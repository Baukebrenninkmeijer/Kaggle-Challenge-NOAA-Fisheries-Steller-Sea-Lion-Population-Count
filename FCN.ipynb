{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Opzet voor netwerk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# read input\n",
    "from patchgenerator import PatchGenerator as pg\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import os\n",
    "from glob import glob\n",
    "\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "matplotlib.rcParams['figure.figsize'] = (20, 12)\n",
    "\n",
    "# neural network\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import lasagne\n",
    "import lasagne.layers as L\n",
    "\n",
    "from IPython import display\n",
    "import time\n",
    "from tqdm import tqdm, tnrange, tqdm_notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Het idee is om uit de data patches te halen op het moment dat deze nodig zijn\n",
    "voor het trainen. Ook wordt er 'data augmentation' toegepast zodat het neurale\n",
    "netwerk niet overfit op de trainin data en beter generaliseert. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_files_path = os.path.join('TrainSmall2', 'Train', '*.jpg')\n",
    "train_files = sorted(glob(train_files_path))\n",
    "\n",
    "coordinates_path = 'coords.csv'\n",
    "csv = pd.read_csv(coordinates_path)\n",
    "\n",
    "# op deze manier gebruiken we alleen labels voor train_files op... \n",
    "files_train = dict()\n",
    "# List of tuples (img, class, (row,col))\n",
    "labels_train = []\n",
    "\n",
    "# 80/20 split van de grote set\n",
    "nr_validation_imgs = int(0.2 * len(train_files))\n",
    "numbers = range(len(train_files))\n",
    "np.random.seed(42)\n",
    "np.random.shuffle(numbers)\n",
    "print('numbers', numbers)\n",
    "validation_imgs = numbers[:nr_validation_imgs]\n",
    "files_validation = dict()\n",
    "labels_validation = []\n",
    "\n",
    "for fullpath in train_files:\n",
    "    prefix, fname = os.path.split(fullpath)\n",
    "    name, ext = os.path.splitext(fname)\n",
    "\n",
    "    file_nr = int(name)\n",
    "\n",
    "    if (file_nr in validation_imgs):\n",
    "        files_validation[file_nr] = fullpath\n",
    "    else:\n",
    "        files_train[file_nr] = fullpath\n",
    "\n",
    "    labels_nr = csv[csv['tid'] == file_nr]\n",
    "\n",
    "    for _, row in labels_nr.iterrows():\n",
    "        label_class = row['cls']\n",
    "        (y, x) = row['col'], row['row']\n",
    "        if (file_nr in validation_imgs):\n",
    "            labels_validation.append((file_nr, label_class, (x, y)))\n",
    "        else:\n",
    "            labels_train.append( (file_nr, label_class, (x, y)) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Building the network\n",
    "\n",
    "Input patch size: 80x80\n",
    "\n",
    "- conv: 78x78\n",
    "- conv: 76x76\n",
    "- pool: 38x38\n",
    "- conv: 36x36\n",
    "- conv: 34x34\n",
    "- conv: 32x32\n",
    "- pool: 16x16\n",
    "- conv: 14x14\n",
    "- conv: 12x12\n",
    "- pool: 6x6\n",
    "- conv: 4x4\n",
    "- conv: 2x2\n",
    "- conv (2x2): 1x1\n",
    "- FC: 1024\n",
    "- FC: 512\n",
    "- FC: 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# assuming an input of 80x80, 3 channels\n",
    "def build_network(input_tensor, nonlinearity=lasagne.nonlinearities.rectify):\n",
    "    network = L.InputLayer(shape=(None, 3, None, None), input_var=input_tensor)\n",
    "\n",
    "    print 'Input shape', network.output_shape\n",
    "\n",
    "    network = L.Conv2DLayer(network, 64, 3, nonlinearity=nonlinearity)\n",
    "    network = L.BatchNormLayer(network)\n",
    "    # 78x78\n",
    "    \n",
    "    network = L.Conv2DLayer(network, 64, 3, nonlinearity=nonlinearity)\n",
    "    network = L.BatchNormLayer(network)\n",
    "    \n",
    "    print 'Before M1', network.output_shape\n",
    "    # 76x76\n",
    "\n",
    "    network = L.MaxPool2DLayer(network, 2)\n",
    "    \n",
    "    print 'After M1', network.output_shape\n",
    "    # 38x38\n",
    "    \n",
    "    network = L.Conv2DLayer(network, 128, 3, nonlinearity=nonlinearity)\n",
    "    network = L.BatchNormLayer(network)\n",
    "    # 36x36\n",
    "    network = L.Conv2DLayer(network, 128, 3, nonlinearity=nonlinearity)\n",
    "    network = L.BatchNormLayer(network)\n",
    "    # 34x34\n",
    "    network = L.Conv2DLayer(network, 128, 3, nonlinearity=nonlinearity)\n",
    "    network = L.BatchNormLayer(network)\n",
    "    # 32x32\n",
    "    \n",
    "    print 'Before M2', network.output_shape\n",
    "    network = L.MaxPool2DLayer(network, 2)\n",
    "    print 'After M2', network.output_shape\n",
    "    # 16x16\n",
    "    \n",
    "    network = L.Conv2DLayer(network, 256, 3, nonlinearity=nonlinearity)\n",
    "    network = L.BatchNormLayer(network)\n",
    "    # 14x14\n",
    "    network = L.Conv2DLayer(network, 256, 3, nonlinearity=nonlinearity)\n",
    "    network = L.BatchNormLayer(network)\n",
    "    # 12x12\n",
    "    \n",
    "    print 'Before M3', network.output_shape\n",
    "    network = L.MaxPool2DLayer(network, 2)\n",
    "    print 'After M3', network.output_shape\n",
    "    # 6x6\n",
    "    \n",
    "    network = L.Conv2DLayer(network, 512, 3, nonlinearity=nonlinearity)\n",
    "    network = L.BatchNormLayer(network)\n",
    "    # 4x4\n",
    "    network = L.Conv2DLayer(network, 512, 3, nonlinearity=nonlinearity)\n",
    "    network = L.BatchNormLayer(network)\n",
    "    # 2x2\n",
    "    network = L.Conv2DLayer(network, 512, 2, nonlinearity=nonlinearity)\n",
    "    network = L.BatchNormLayer(network)\n",
    "    # 1x1\n",
    "    print 'Before fully conv', network.output_shape\n",
    "    \n",
    "    network = L.Conv2DLayer(network, 1024, 1, nonlinearity=nonlinearity)\n",
    "    network = L.BatchNormLayer(network)\n",
    "    network = L.Conv2DLayer(network, 512, 1, nonlinearity=nonlinearity)\n",
    "    # 2 classes\n",
    "    network = L.Conv2DLayer(network, 2, 1, nonlinearity=nonlinearity)    \n",
    "    \n",
    "    print 'Final output', network.output_shape\n",
    "        \n",
    "    return network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# assuming an input of 40x40, 3 channels\n",
    "def build_network_small(input_tensor, nonlinearity=lasagne.nonlinearities.rectify):\n",
    "    network = L.InputLayer(shape=(None, 3, None, None), input_var=input_tensor)\n",
    "\n",
    "    print 'Input shape', network.output_shape\n",
    "\n",
    "    network = L.Conv2DLayer(network, 64, 3, nonlinearity=nonlinearity, W=lasagne.init.GlorotUniform())\n",
    "    # 38x38\n",
    "    \n",
    "    network = L.Conv2DLayer(network, 64, 3, nonlinearity=nonlinearity,W=lasagne.init.GlorotUniform())\n",
    "    network = L.BatchNormLayer(network)\n",
    "    # 36x36\n",
    "    network = L.Conv2DLayer(network, 64, 3, nonlinearity=nonlinearity,W=lasagne.init.GlorotUniform())\n",
    "    network = L.BatchNormLayer(network)\n",
    "    # 34x34\n",
    "    network = L.Conv2DLayer(network, 64, 3, nonlinearity=nonlinearity,W=lasagne.init.GlorotUniform())\n",
    "    network = L.BatchNormLayer(network)\n",
    "    # 32x32\n",
    "    \n",
    "    print 'Before M2', network.output_shape\n",
    "    network = L.MaxPool2DLayer(network, 2)\n",
    "    print 'After M2', network.output_shape\n",
    "    # 16x16\n",
    "    \n",
    "    network = L.Conv2DLayer(network, 128, 3, nonlinearity=nonlinearity,W=lasagne.init.GlorotUniform())\n",
    "    network = L.BatchNormLayer(network)\n",
    "    # 14x14\n",
    "    network = L.Conv2DLayer(network, 128, 3, nonlinearity=nonlinearity,W=lasagne.init.GlorotUniform())\n",
    "    network = L.BatchNormLayer(network)\n",
    "    # 12x12\n",
    "    \n",
    "    print 'Before M3', network.output_shape\n",
    "    network = L.MaxPool2DLayer(network, 2)\n",
    "    print 'After M3', network.output_shape\n",
    "    # 6x6\n",
    "    \n",
    "    network = L.Conv2DLayer(network, 256, 3, nonlinearity=nonlinearity,W=lasagne.init.GlorotUniform())\n",
    "    network = L.BatchNormLayer(network)\n",
    "    # 4x4\n",
    "    network = L.Conv2DLayer(network, 256, 3, nonlinearity=nonlinearity,W=lasagne.init.GlorotUniform())\n",
    "    network = L.BatchNormLayer(network)\n",
    "    # 2x2\n",
    "    network = L.Conv2DLayer(network, 256, 2, nonlinearity=nonlinearity,W=lasagne.init.GlorotUniform())\n",
    "    network = L.BatchNormLayer(network)\n",
    "    # 1x1\n",
    "    print 'Before fully conv', network.output_shape\n",
    "    \n",
    "    network = L.Conv2DLayer(network, 512, 1, nonlinearity=nonlinearity,W=lasagne.init.GlorotUniform())\n",
    "    network = L.BatchNormLayer(network)\n",
    "    network = L.Conv2DLayer(network, 256, 1, nonlinearity=nonlinearity,W=lasagne.init.GlorotUniform())\n",
    "    # 2 classes\n",
    "    network = L.Conv2DLayer(network, 2, 1, nonlinearity=nonlinearity,W=lasagne.init.GlorotUniform())    \n",
    "    \n",
    "    print 'Final output', network.output_shape\n",
    "        \n",
    "    return network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# assuming an input of 120x120x3 \n",
    "def build_network_large(input_tensor, nonlinearity=lasagne.nonlinearities.rectify):\n",
    "    network = L.InputLayer(shape=(None, 3, None, None), input_var=input_tensor)\n",
    "    # 120x120\n",
    "    network = L.Conv2DLayer(network, 64, 3, nonlinearity=nonlinearity,W=lasagne.init.GlorotUniform())\n",
    "    network = L.BatchNormLayer(network)\n",
    "    # 118x118\n",
    "    network = L.Conv2DLayer(network, 64, 3, nonlinearity=nonlinearity,W=lasagne.init.GlorotUniform())\n",
    "    network = L.BatchNormLayer(network)\n",
    "    # 116x116\n",
    "    network = L.MaxPool2DLayer(network, 2) # M1\n",
    "    # 58x58\n",
    "    network = L.Conv2DLayer(network, 64, 3, nonlinearity=nonlinearity,W=lasagne.init.GlorotUniform())\n",
    "    network = L.BatchNormLayer(network)\n",
    "    # 56x56\n",
    "    network = L.Conv2DLayer(network, 64, 3, nonlinearity=nonlinearity,W=lasagne.init.GlorotUniform())\n",
    "    network = L.BatchNormLayer(network)\n",
    "    # 54x54\n",
    "    network = L.Conv2DLayer(network, 64, 3, nonlinearity=nonlinearity,W=lasagne.init.GlorotUniform())\n",
    "    network = L.BatchNormLayer(network)\n",
    "    # 52x52\n",
    "    network = L.MaxPool2DLayer(network, 2) # M2\n",
    "    # 26x26\n",
    "    network = L.Conv2DLayer(network, 128, 3, nonlinearity=nonlinearity,W=lasagne.init.GlorotUniform())\n",
    "    network = L.BatchNormLayer(network)\n",
    "    # 24x24\n",
    "    network = L.Conv2DLayer(network, 128, 3, nonlinearity=nonlinearity,W=lasagne.init.GlorotUniform())\n",
    "    network = L.BatchNormLayer(network)\n",
    "    # 22x22\n",
    "    network = L.Conv2DLayer(network, 128, 3, nonlinearity=nonlinearity,W=lasagne.init.GlorotUniform())\n",
    "    network = L.BatchNormLayer(network)\n",
    "    # 20x20\n",
    "    network = L.MaxPool2DLayer(network, 2) # M3\n",
    "    # 10x10\n",
    "    network = L.Conv2DLayer(network, 128, 3, nonlinearity=nonlinearity,W=lasagne.init.GlorotUniform())\n",
    "    network = L.BatchNormLayer(network)\n",
    "    # 8x8\n",
    "    network = L.Conv2DLayer(network, 128, 3, nonlinearity=nonlinearity,W=lasagne.init.GlorotUniform())\n",
    "    network = L.BatchNormLayer(network)\n",
    "    # 6x6\n",
    "    network = L.Conv2DLayer(network, 256, 3, nonlinearity=nonlinearity,W=lasagne.init.GlorotUniform())\n",
    "    network = L.BatchNormLayer(network)\n",
    "    # 4x4\n",
    "    network = L.Conv2DLayer(network, 512, 4, nonlinearity=nonlinearity,W=lasagne.init.GlorotUniform())\n",
    "    network = L.BatchNormLayer(network)\n",
    "    # 1x1\n",
    "    network = L.Conv2DLayer(network, 512, 1, nonlinearity=nonlinearity,W=lasagne.init.GlorotUniform())\n",
    "    # 1x1\n",
    "    network = L.Conv2DLayer(network, 2, 1, nonlinearity=nonlinearity,W=lasagne.init.GlorotUniform())\n",
    "    \n",
    "    return network\n",
    "\n",
    "params_file = './parameters-FCN-large.npz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def softmax(network):\n",
    "    output = lasagne.layers.get_output(network)\n",
    "    exp = T.exp(output - output.max(axis=1, keepdims=True)) #subtract max for numeric stability (overflow)\n",
    "    return exp / exp.sum(axis=1, keepdims=True)\n",
    "\n",
    "def softmax_deterministic(network):\n",
    "    output = lasagne.layers.get_output(network, deterministic=True)\n",
    "    exp = T.exp(output - output.max(axis=1, keepdims=True)) #subtract max for numeric stability (overflow)\n",
    "    return exp / exp.sum(axis=1, keepdims=True)\n",
    "\n",
    "def log_softmax(network):\n",
    "    output = lasagne.layers.get_output(network)\n",
    "    xdev = output - output.max(1, keepdims=True)\n",
    "    return xdev - T.log(T.sum(T.exp(xdev), axis=1, keepdims=True))\n",
    "\n",
    "def log_softmax_deterministic(network):\n",
    "    output = lasagne.layers.get_output(network, deterministic=True)\n",
    "    xdev = output - output.max(1, keepdims=True)\n",
    "    return xdev - T.log(T.sum(T.exp(xdev), axis=1, keepdims=True))\n",
    "\n",
    "def categorical_crossentropy_logdomain(log_predictions, targets):\n",
    "    return -T.sum(targets * log_predictions, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def training_function(network, input_tensor, target_tensor, learning_rate, use_l2_regularization=True, l2_lambda=0.000001):\n",
    "    # Get the network output and calculate metrics.\n",
    "    network_output = softmax(network)\n",
    "        \n",
    "    if use_l2_regularization:\n",
    "        l2_loss = lasagne.regularization.regularize_network_params(network, lasagne.regularization.l2)\n",
    "        loss = lasagne.objectives.categorical_crossentropy(network_output, target_tensor).mean() + l2_lambda * l2_loss\n",
    "    else:\n",
    "        loss = lasagne.objectives.categorical_crossentropy(network_output, target_tensor).mean()\n",
    "        \n",
    "    accuracy = T.mean(T.eq(T.argmax(network_output, axis=1), T.argmax(target_tensor,axis=1)), dtype=theano.config.floatX)\n",
    "    \n",
    "    # Get the network parameters and the update function.                      \n",
    "    network_params = L.get_all_params(network, trainable=True)\n",
    "    weight_updates = lasagne.updates.adam(loss, network_params, learning_rate=learning_rate)\n",
    "    \n",
    "    # Construct the training function.\n",
    "    return theano.function([input_tensor, target_tensor], [loss, accuracy], updates=weight_updates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def validate_function(network, input_tensor, target_tensor):\n",
    "    # Get the network output and calculate metrics.\n",
    "    network_output = softmax_deterministic(network)\n",
    "    loss = lasagne.objectives.categorical_crossentropy(network_output, target_tensor).mean()\n",
    "    accuracy = T.mean(T.eq(T.argmax(network_output, axis=1), T.argmax(target_tensor,axis=1)), dtype=theano.config.floatX)  \n",
    "    \n",
    "    # Construct the validation function.\n",
    "    return theano.function([input_tensor, target_tensor], [loss, accuracy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate_function(network, input_tensor):\n",
    "    # Get the network output and calculate metrics.\n",
    "    network_output = softmax_deterministic(network)\n",
    "    \n",
    "    # Construct the evaluation function.\n",
    "    return theano.function([input_tensor], network_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# define input (batch_size, channels, x, y)\n",
    "input_var = T.tensor4('inputs')\n",
    "\n",
    "# define targets (batch_size, 2, 1, 1)\n",
    "target_var = T.tensor4('targets')\n",
    "\n",
    "\n",
    "# network = build_network(input_var, lasagne.nonlinearities.linear)\n",
    "# params_file = './parameters-FCN.npz'\n",
    "\n",
    "# network = build_network_small(input_var, lasagne.nonlinearities.linear)\n",
    "# params_file = './parameters-FCN-small.npz'\n",
    "\n",
    "network = build_network_large(input_var, lasagne.nonlinearities.rectify)\n",
    "\n",
    "# load stored params?\n",
    "load_stored_params = True\n",
    "\n",
    "if load_stored_params:\n",
    "    npz = np.load(params_file)\n",
    "    L.set_all_param_values(network, npz['params'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Patch Generator init function:\n",
    "# pg(files, labels, batch_size, num_classes=2, patch_size=140, random_flipping=False, max_shift=0, floatX = 'float32')\n",
    "\n",
    "patch_size = 120\n",
    "max_shift = patch_size // 10\n",
    "flipping = True\n",
    "\n",
    "batch_size = 40\n",
    "\n",
    "# validate on 10 batches\n",
    "\n",
    "\n",
    "pg_train = pg(files_train, \n",
    "              labels_train, \n",
    "              batch_size, \n",
    "              num_classes=2, \n",
    "              patch_size=patch_size, \n",
    "              random_flipping=flipping, \n",
    "              max_shift=max_shift,\n",
    "              floatX = theano.config.floatX)\n",
    "\n",
    "validation_batch_size = 40\n",
    "\n",
    "pg_val = pg(files_validation,\n",
    "            labels_validation,\n",
    "            validation_batch_size,\n",
    "            num_classes=2,\n",
    "            patch_size=patch_size,\n",
    "            random_flipping=flipping,\n",
    "            max_shift=max_shift,\n",
    "            floatX = theano.config.floatX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Initialize functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "\n",
    "train_fn = training_function(network=network, input_tensor=input_var, target_tensor=target_var, learning_rate=learning_rate,\n",
    "                             use_l2_regularization=True, l2_lambda=0.000001)\n",
    "validation_fn = validate_function(network=network, input_tensor=input_var, target_tensor=target_var)\n",
    "# evaluation_fn = evaluate_function(network=network, input_tensor=input_var) # define when we actually need it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Sanity check, label distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# manually count the label classes\n",
    "counts = dict()\n",
    "for label in pg_train.labels:\n",
    "    clss = label[1]\n",
    "    if clss not in counts:\n",
    "        counts[clss] = 0\n",
    "    counts[clss] += 1\n",
    "\n",
    "print counts\n",
    "print counts.keys()\n",
    "print counts.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# do we want a nice graph? yes we do\n",
    "\n",
    "plt.figure()\n",
    "plt.title('Class distribution in training set')\n",
    "\n",
    "indices = range(len(counts))\n",
    "\n",
    "plt.bar(indices, counts.values())\n",
    "plt.xticks(indices, counts.keys() ) #, rotation=45, ha='right') # if we want text labels, these last two might be left in\n",
    "plt.grid(True, axis='y')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# same goes here, but for the validation set\n",
    "counts = dict()\n",
    "for label in pg_val.labels:\n",
    "    clss = label[1]\n",
    "    if clss not in counts:\n",
    "        counts[clss] = 0\n",
    "    counts[clss] += 1\n",
    "\n",
    "print counts\n",
    "print counts.keys()\n",
    "print counts.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.title('Class distribution in validation set')\n",
    "\n",
    "indices = range(len(counts))\n",
    "\n",
    "plt.bar(indices, counts.values())\n",
    "plt.xticks(indices, counts.keys() ) #, rotation=45, ha='right') # if we want text labels, these last two might be left in\n",
    "plt.grid(True, axis='y')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nr_epochs = 10\n",
    "nr_iterations_per_epoch = 40 # used for training as well as validation\n",
    "\n",
    "tra_loss_lst = []\n",
    "val_loss_lst= []\n",
    "tra_acc_lst = []\n",
    "val_acc_lst = []\n",
    "best_val_acc = 0.9625\n",
    "\n",
    "# training loop\n",
    "print 'started training with learning rate', learning_rate\n",
    "\n",
    "for epoch in range(1, nr_epochs+1):\n",
    "    print 'Epoch {}'.format(epoch)\n",
    "    \n",
    "    training_loss = []\n",
    "    training_accuracy = []\n",
    "    \n",
    "    print 'training...'\n",
    "    for mini_batch in tqdm_notebook(range(nr_iterations_per_epoch), leave=False):\n",
    "        inputs, targets = next(pg_train)\n",
    "        \n",
    "        # Reshape targets from (batch_size, 2) to (batch_size,2,1,1) in order to match network output\n",
    "        targets = targets.reshape(batch_size, 2, 1, 1)\n",
    "        \n",
    "        # something goes wrong here\n",
    "        loss, accuracy = train_fn(inputs, targets)\n",
    "        \n",
    "        training_loss.append(loss)\n",
    "        training_accuracy.append(accuracy)\n",
    "        \n",
    "#         print(\"Loss: {}, accuracy: {}\".format(loss,accuracy))\n",
    "    \n",
    "    tra_loss_lst.append(np.mean(training_loss))\n",
    "    tra_acc_lst.append(np.mean(training_accuracy))\n",
    "    print 'mean training loss', np.mean(training_loss)\n",
    "    print 'mean training accuracy', np.mean(training_accuracy)\n",
    "    \n",
    "    val_losses = []\n",
    "    val_accs = []\n",
    "    \n",
    "    print 'validating...'\n",
    "    # validate each epoch\n",
    "    for mini_batch in tqdm_notebook(range(nr_iterations_per_epoch), leave=False):\n",
    "        inputs, targets = next(pg_val)\n",
    "        targets = targets.reshape(validation_batch_size,2,1,1)\n",
    "        loss, accuracy = validation_fn(inputs, targets)\n",
    "        val_losses.append(loss)\n",
    "        val_accs.append(accuracy)\n",
    "    \n",
    "    val_loss_lst.append(np.mean(val_losses))\n",
    "    val_acc_lst.append(np.mean(val_accs))\n",
    "    print 'mean validation loss', np.mean(val_accs)\n",
    "    print 'mean validation accuracy', np.mean(val_accs)\n",
    "    #continue\n",
    "    if np.mean(val_accs) >= best_val_acc:\n",
    "        best_val_acc = np.mean(val_accs)\n",
    "        # save network\n",
    "        params = L.get_all_param_values(network)\n",
    "        np.savez(params_file, params=params)\n",
    "\n",
    "    # plot learning curves\n",
    "    fig = plt.figure()\n",
    "#     tra_loss_plt, = plt.plot(tra_loss_lst, 'b-')\n",
    "#     val_loss_plt, = plt.plot(val_loss_lst, 'g-')\n",
    "    tra_acc_plt, = plt.plot(tra_acc_lst, 'b')\n",
    "    val_acc_plt, = plt.plot(val_acc_lst, 'g')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('loss')\n",
    "    plt.legend([#tra_loss_plt, val_loss_plt, \n",
    "                tra_acc_plt, val_acc_plt], \n",
    "               [#'training loss', 'validation loss', \n",
    "                'training accuracy', 'validation accuracy'],\n",
    "                loc='center left', bbox_to_anchor=(1, .9)\n",
    "              )\n",
    "    plt.title('Best validation accuracy = {:.2f}%'.format(100. * best_val_acc))\n",
    "    plt.grid()\n",
    "    display.clear_output(wait=True)\n",
    "    display.display(plt.gcf())\n",
    "    time.sleep(.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# save anyway?\n",
    "params = L.get_all_param_values(network)\n",
    "np.savez('doorgetraind.npz', params=params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Load test images and apply the network to them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load network and its parameters\n",
    "input_var = T.tensor4('inputs', dtype=theano.config.floatX)\n",
    "target_var = T.tensor4('targets', dtype=theano.config.floatX)\n",
    "\n",
    "# network = build_network(input_var, lasagne.nonlinearities.rectify)\n",
    "# network = build_network_small(input_var, lasagne.nonlinearities.rectify)\n",
    "network = build_network_large(input_var, lasagne.nonlinearities.rectify)\n",
    "patch_size = 120\n",
    "\n",
    "evaluation_fn = evaluate_function(network=network, input_tensor=input_var)\n",
    "\n",
    "# load stored params\n",
    "npz = np.load(params_file)\n",
    "L.set_all_param_values(network, npz['params'])\n",
    "\n",
    "threshold = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# compute probability maps (segmentations) on the test images\n",
    "from PIL import Image\n",
    "\n",
    "test_files_path = os.path.join('D:', '*.jpg')\n",
    "# test_files_path = os.path.join('Train', '*.jpg')\n",
    "test_images = sorted(glob(test_files_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# set to True if we want some pretty plots for a presentation or whatever\n",
    "plot = False\n",
    "debug = False\n",
    "save_output = True\n",
    "save_directory = os.path.join('D:', 'Test', 'results', 'probmaps')\n",
    "# save_directory = os.path.join('results', 'Train', 'probmaps')\n",
    "\n",
    "for img_fullpath in test_images:\n",
    "    # img id (number)\n",
    "    _, tail = os.path.split(img_fullpath)\n",
    "    img_id, _ = os.path.splitext(tail)\n",
    "    \n",
    "    fname = 'probability-map-{}.npz'.format(img_id)\n",
    "    if os.path.exists(os.path.join(save_directory, fname)):\n",
    "        # skip if we made this one already\n",
    "        continue\n",
    "    \n",
    "    print 'Loading image', tail\n",
    "    img = Image.open(img_fullpath)\n",
    "    img = np.array(img)\n",
    "    img = img / 255.0\n",
    "    if debug:\n",
    "        print 'image id', img_id\n",
    "        print('image shape {}'.format(img.shape))\n",
    "        \n",
    "    A, B, C = img.shape\n",
    "    \n",
    "    # divide the images in smaller segments to fit the GPUs memory\n",
    "    A_STEP = A // 4\n",
    "    B_STEP = B // 4\n",
    "    \n",
    "    # to store probability maps in\n",
    "    probability_map = dict()\n",
    "    \n",
    "    stime = time.time()\n",
    "    \n",
    "    ia = 0\n",
    "    for a in range(0, A, A_STEP):\n",
    "        ib = 0\n",
    "        for b in range(0, B, B_STEP):\n",
    "            # take a BIG_STRIDE x BIG_STRIDE area of the image.\n",
    "            if debug:\n",
    "                print 'a', a, 'b', b\n",
    "            img_slice = img[a:a+A_STEP, b:b+B_STEP, :].astype(np.float32)\n",
    "\n",
    "            # reshape into \n",
    "            # channels, dimension1, dimension2\n",
    "            cdd = img_slice.transpose( (2,0,1) )\n",
    "\n",
    "            # bij gebrek aan betere ideeën, padding over 3 channels\n",
    "            img_padded_r = np.pad(cdd[0,:,:], patch_size // 2, 'constant', constant_values=0)\n",
    "            img_padded_g = np.pad(cdd[1,:,:], patch_size // 2, 'constant', constant_values=0)\n",
    "            img_padded_b = np.pad(cdd[2,:,:], patch_size // 2, 'constant', constant_values=0)\n",
    "    \n",
    "            img_padded = np.array([img_padded_r, img_padded_g, img_padded_b])\n",
    "            img_padded = np.expand_dims(img_padded, axis=0)\n",
    "            \n",
    "            if debug:\n",
    "                print('computing probability map...')\n",
    "            t = -time.time()\n",
    "            # normal method\n",
    "            probability = evaluation_fn(img_padded)\n",
    "            preds = probability[0,1,:,:]\n",
    "            t += time.time()\n",
    "            if debug:\n",
    "                print('computed probability map in {} seconds'.format(t))\n",
    "\n",
    "            # collect the probability map in the lookup table\n",
    "            probability_map[ (ia, ib) ] = preds\n",
    "            \n",
    "            ib += 1\n",
    "        ia += 1\n",
    "    \n",
    "    etime = time.time()\n",
    "    print 'it took {} seconds to compute the probability map'.format(etime - stime)\n",
    "    \n",
    "# simple upscale using Kronecker\n",
    "# could be replaced by shift and stitch, but a perfect segmentation isn't really our goal\n",
    "#             n = 8\n",
    "#             preds = np.kron(preds, np.ones((n,n)))\n",
    "\n",
    "    # extract the shape of the complete probability map\n",
    "    shape = [0, 0]\n",
    "    As = {}\n",
    "    Bs = {}\n",
    "    for key in probability_map.keys():\n",
    "        a, b = key\n",
    "        pms = probability_map[key].shape\n",
    "        if not (a in As):\n",
    "            shape[0] += pms[0]\n",
    "            As[a] = True\n",
    "        \n",
    "        if not (b in Bs):\n",
    "            shape[1] += pms[1]\n",
    "            Bs[b] = True\n",
    "\n",
    "    PMAP = np.zeros( (shape[0], shape[1]) )\n",
    "    if debug:\n",
    "        print 'probability map shape', PMAP.shape\n",
    "    \n",
    "    # order the keys\n",
    "    keys = probability_map.keys()\n",
    "    keys = sorted(keys, key=lambda t: (t[0], t[1]))\n",
    "    if debug:\n",
    "        print 'keys', keys\n",
    "    \n",
    "    # idea: iterate through keys, keep track of probability map coordinates for each value a/b\n",
    "    saA = { 0:0 }\n",
    "    saB = { 0:0 }\n",
    "    for a, b in keys:\n",
    "        # keys are sorted [(0,0), (0,1) ..]\n",
    "        pm = probability_map[(a,b)]\n",
    "        pms = pm.shape\n",
    "        if not ((a+1) in saA):\n",
    "            saA[(a+1)] = saA[a] + pms[0]\n",
    "        if not ((b+1) in saB):\n",
    "            saB[(b+1)] = saB[b] + pms[1]\n",
    "        \n",
    "        if debug:\n",
    "            print 'a, b', a, b\n",
    "            print 'PMAP[..].shape', PMAP[saA[a]:saA[a+1], saB[b]:saB[b+1]].shape\n",
    "            print 'pms', pms\n",
    "            \n",
    "        PMAP[saA[a]:saA[a+1], saB[b]:saB[b+1]] = pm\n",
    "\n",
    "    # plot whole image\n",
    "#     plt.figure(figsize=(16,20))\n",
    "#     plt.imshow(img, interpolation='none') #.transpose( (2,0,1) ))\n",
    "#     plt.figure(figsize=(16,20))\n",
    "#     plt.imshow(PMAP, alpha=0.8, interpolation='none', cmap='viridis')\n",
    "#     plt.title('Image with probability map')\n",
    "#     plt.show()\n",
    "\n",
    "    # save output\n",
    "    if save_output:\n",
    "        if not os.path.exists(save_directory):\n",
    "            os.makedirs(save_directory)\n",
    "\n",
    "        # savez\n",
    "        fname = 'probability-map-{}.npz'.format(img_id)\n",
    "        np.savez(os.path.join(save_directory, fname), PMAP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# further stuff?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
